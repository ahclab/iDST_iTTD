{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import logging\n",
    "import math\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "from tqdm import tqdm\n",
    "from idst_util import trivial\n",
    "from idst_util import dstc2\n",
    "\n",
    "# Make sure data is available\n",
    "trivial.print_idst()\n",
    "dstc2.check()\n",
    "\n",
    "# Retrieve raw data\n",
    "raw_X_train, raw_Y_train, \\\n",
    "raw_X_dev, raw_Y_dev, \\\n",
    "raw_X_test, raw_X_test, \\\n",
    "ontology = dstc2.retrieve_raw_datasets(train_data_augmentation = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"+--------------------------------+\")\n",
    "logging.info(\"|            Baseline            |\")\n",
    "logging.info(\"+--------------------------------+\")\n",
    "\n",
    "#GPU_ID = 0\n",
    "#DEVICE = torch.device(\"cuda:{}\".format(GPU_ID) if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE = \"cpu\"\n",
    "if DEVICE == \"cpu\":\n",
    "    logging.warning(\"Running on CPU\")\n",
    "else:\n",
    "    logging.info(\"Running on GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"+--------------------------------+\")\n",
    "logging.info(\"|          Vocabulary            |\")\n",
    "logging.info(\"+--------------------------------+\")\n",
    "logging.info(\"Creating token_to_index, index_to_token and token_to_count dictionaries\")\n",
    "\n",
    "token_to_index = {\"<unk>\": 0}\n",
    "index_to_token = {0: \"<unk>\"}\n",
    "token_to_count = {\"<unk>\": 1}\n",
    "max_sequence_length = 0\n",
    "\n",
    "for raw_train_dialog in tqdm(raw_X_train):\n",
    "    for raw_train_turn in raw_train_dialog:\n",
    "        \n",
    "        current_sequence_length = len(raw_train_turn[\"system\"]) + len(raw_train_turn[\"user\"])\n",
    "        if current_sequence_length > max_sequence_length:\n",
    "            max_sequence_length = current_sequence_length\n",
    "        \n",
    "        for system_token in raw_train_turn[\"system\"]:\n",
    "            token = system_token[0]\n",
    "            if token not in token_to_index:\n",
    "                token_to_index[token] = len(token_to_index)\n",
    "                index_to_token[len(token_to_index)] = token\n",
    "                token_to_count[token] = 1\n",
    "            else:\n",
    "                token_to_count[token] += 1\n",
    "        \n",
    "        for user_token in raw_train_turn[\"user\"]:\n",
    "            token = user_token[0]\n",
    "            if token not in token_to_index:\n",
    "                token_to_index[token] = len(token_to_index)\n",
    "                index_to_token[len(token_to_index)] = token\n",
    "                token_to_count[token] = 1\n",
    "            else:\n",
    "                token_to_count[token] += 1\n",
    "                \n",
    "assert len(token_to_index) == len(index_to_token)\n",
    "assert len(token_to_index) == len(token_to_count)\n",
    "\n",
    "logging.info(\"Vocabulary length:\\t{}\".format(len(token_to_index)))\n",
    "logging.info(\"Max sequence length:\\t{}\".format(max_sequence_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"+--------------------------------+\")\n",
    "logging.info(\"|         Configuration          |\")\n",
    "logging.info(\"+--------------------------------+\")\n",
    "\n",
    "VOCABULARY_SIZE = len(token_to_index)\n",
    "MAX_SEQUENCE_LENGTH = max_sequence_length\n",
    "EMBEDDING_DIM = 170\n",
    "HIDDEN_DIM = 100\n",
    "NUM_NODES = 300\n",
    "\n",
    "METHOD_DIM = len(ontology[\"method\"])\n",
    "REQUESTED_DIM = int(math.pow(2, len(ontology[\"requestable\"])))\n",
    "GOAL_FOOD_DIM = len(ontology[\"informable\"][\"food\"]) + 1\n",
    "GOAL_PRICERANGE_DIM = len(ontology[\"informable\"][\"pricerange\"]) + 1\n",
    "GOAL_NAME_DIM = len(ontology[\"informable\"][\"name\"]) + 1\n",
    "GOAL_AREA_DIM = len(ontology[\"informable\"][\"area\"]) + 1\n",
    "\n",
    "logging.info(\"VOCABULARY_SIZE:\\t{}\".format(VOCABULARY_SIZE))\n",
    "logging.info(\"MAX_SEQUENCE_LENGTH:\\t{}\".format(MAX_SEQUENCE_LENGTH))\n",
    "logging.info(\"EMBEDDING_DIM:\\t{}\".format(EMBEDDING_DIM))\n",
    "logging.info(\"HIDDEN_DIM:\\t\\t{}\".format(HIDDEN_DIM))\n",
    "logging.info(\"NUM_NODES:\\t\\t{}\".format(NUM_NODES))\n",
    "logging.info(\"METHOD_DIM:\\t\\t{}\".format(METHOD_DIM))\n",
    "logging.info(\"REQUESTED_DIM:\\t{}\".format(REQUESTED_DIM))\n",
    "logging.info(\"GOAL_FOOD_DIM:\\t{}\".format(GOAL_FOOD_DIM))\n",
    "logging.info(\"GOAL_PRICERANGE_DIM:\\t{}\".format(GOAL_PRICERANGE_DIM))\n",
    "logging.info(\"GOAL_NAME_DIM:\\t{}\".format(GOAL_NAME_DIM))\n",
    "logging.info(\"GOAL_AREA_DIM:\\t{}\".format(GOAL_AREA_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LecTrackEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, hidden_dim, vocabulary_size, num_nodes):\n",
    "        super(LecTrackEncoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.token_embeddings = nn.Embedding(num_embeddings = vocabulary_size,\n",
    "                                            embedding_dim = embedding_dim)\n",
    "        self.linear = nn.Linear(in_features = (embedding_dim + 1),\n",
    "                                out_features = num_nodes)\n",
    "        self.lstm = nn.LSTM(num_nodes, hidden_dim)\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(1, 1, self.hidden_dim), torch.zeros(1, 1, self.hidden_dim))\n",
    "        \n",
    "    def forward(self, indexes, scores):\n",
    "        embeddings = self.token_embeddings(indexes)\n",
    "        embeddings_concat_score = torch.cat((embeddings, scores.unsqueeze(dim = 1)), dim = 1) \n",
    "        altered_embeddings = F.relu(self.linear(embeddings_concat_score))\n",
    "        lstm_out, self.hidden = self.lstm(altered_embeddings.view(len(indexes), 1, -1), self.hidden)\n",
    "        return self.hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LecTrackMethodClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, method_dim):\n",
    "        super(LecTrackMethodClassifier, self).__init__()\n",
    "        self.linear = nn.Linear(in_features = hidden_dim,\n",
    "                                out_features = method_dim)\n",
    "        \n",
    "    def forward(self, hidden):\n",
    "        return F.log_softmax(self.linear(hidden), dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LecTrackRequestedClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, requested_dim):\n",
    "        super(LecTrackRequestedClassifier, self).__init__()\n",
    "        self.linear = nn.Linear(in_features = hidden_dim,\n",
    "                                out_features = requested_dim) \n",
    "        \n",
    "    def forward(self, hidden):\n",
    "        return F.log_softmax(self.linear(hidden), dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LecTrackGoalFoodClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, goal_food_dim):\n",
    "        super(LecTrackGoalFoodClassifier, self).__init__()\n",
    "        self.linear = nn.Linear(in_features = hidden_dim,\n",
    "                                out_features = goal_food_dim)\n",
    "        \n",
    "    def forward(self, hidden):\n",
    "        return F.log_softmax(self.linear(hidden), dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LecTrackGoalPricerangeClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, goal_pricerange_dim):\n",
    "        super(LecTrackGoalPricerangeClassifier, self).__init__()\n",
    "        self.linear = nn.Linear(in_features = hidden_dim,\n",
    "                                out_features = goal_pricerange_dim)\n",
    "        \n",
    "    def forward(self, hidden):\n",
    "        return F.log_softmax(self.linear(hidden), dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LecTrackGoalNameClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, goal_name_dim):\n",
    "        super(LecTrackGoalNameClassifier, self).__init__()\n",
    "        self.linear = nn.Linear(in_features = hidden_dim,\n",
    "                                out_features = goal_name_dim)\n",
    "        \n",
    "    def forward(self, hidden):\n",
    "        return F.log_softmax(self.linear(hidden), dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LecTrackGoalAreaClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, goal_area_dim):\n",
    "        super(LecTrackGoalAreaClassifier, self).__init__()\n",
    "        self.linear = nn.Linear(in_features = hidden_dim,\n",
    "                                out_features = goal_area_dim)\n",
    "        \n",
    "    def forward(self, hidden):\n",
    "        return F.log_softmax(self.linear(hidden), dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LecTrackEncoder = LecTrackEncoder(embedding_dim = EMBEDDING_DIM,\n",
    "                                        hidden_dim = HIDDEN_DIM,\n",
    "                                        vocabulary_size = VOCABULARY_SIZE,\n",
    "                                        num_nodes = NUM_NODES)\n",
    "\n",
    "model_LecTrackMethodClassifier = LecTrackMethodClassifier(hidden_dim = HIDDEN_DIM * 2,\n",
    "                                                          method_dim = METHOD_DIM)\n",
    "\n",
    "model_LecTrackRequestedClassifier = LecTrackRequestedClassifier(hidden_dim = HIDDEN_DIM * 2,\n",
    "                                                                requested_dim = REQUESTED_DIM)\n",
    "\n",
    "model_LecTrackGoalFoodClassifier = LecTrackGoalFoodClassifier(hidden_dim = HIDDEN_DIM * 2,\n",
    "                                                              goal_food_dim = GOAL_FOOD_DIM)\n",
    "\n",
    "model_LecTrackGoalPricerangeClassifier = LecTrackGoalPricerangeClassifier(hidden_dim = HIDDEN_DIM * 2,\n",
    "                                                                          goal_pricerange_dim = GOAL_PRICERANGE_DIM)\n",
    "\n",
    "model_LecTrackGoalNameClassifier = LecTrackGoalNameClassifier(hidden_dim = HIDDEN_DIM * 2,\n",
    "                                                             goal_name_dim = GOAL_NAME_DIM)\n",
    "\n",
    "model_LecTrackGoalAreaClassifier = LecTrackGoalAreaClassifier(hidden_dim = HIDDEN_DIM * 2,\n",
    "                                                             goal_area_dim = GOAL_AREA_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_and_score(turn, token_to_index):\n",
    "    indexes = []\n",
    "    scores = []\n",
    "    token_score_list = turn[\"system\"] + turn[\"user\"]\n",
    "    for token, score in token_score_list:\n",
    "        if token not in token_to_index:\n",
    "            indexes.append(token_to_index[\"<unk>\"])\n",
    "        else:\n",
    "            indexes.append(token_to_index[token])\n",
    "        scores.append(score)\n",
    "    assert len(indexes) == len(scores)\n",
    "    return torch.tensor(indexes, dtype = torch.long), torch.tensor(scores, dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for raw_train_dialog in tqdm(raw_X_train):\n",
    "    for raw_train_turn in raw_train_dialog:\n",
    "        indexes, scores = get_index_and_score(raw_train_turn, token_to_index)\n",
    "        hidden = model_LecTrackEncoder(indexes, scores)\n",
    "        print(hidden)\n",
    "        break\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
